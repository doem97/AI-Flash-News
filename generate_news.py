#!/usr/bin/env python3
"""
AI æ–°é—»ç®€æŠ¥è‡ªåŠ¨ç”Ÿæˆè„šæœ¬
ä½¿ç”¨ Claude API æŠ“å–ã€åˆ†æå’Œç”Ÿæˆæ¯æ—¥ AI æ–°é—»ç®€æŠ¥ï¼ˆJSONæ ¼å¼ï¼‰
"""

import os
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path


def get_historical_news_json(days=7):
    """
    è¯»å–æœ€è¿‘Nå¤©çš„å†å²JSONæ–°é—»æ–‡ä»¶

    Args:
        days: è¦å›æº¯çš„å¤©æ•°

    Returns:
        åŒ…å«å†å²æ–°é—»çš„åˆ—è¡¨
    """
    current_dir = Path(".")
    historical_data = []

    # è·å–æœ€è¿‘Nå¤©çš„æ—¥æœŸ
    for i in range(1, days + 1):
        date = datetime.now() - timedelta(days=i)
        filename = date.strftime("%Y-%m-%d.json")
        filepath = current_dir / filename

        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    historical_data.append({
                        "date": data.get("date", filename.replace(".json", "")),
                        "news": data.get("news", [])
                    })
            except json.JSONDecodeError:
                print(f"âš ï¸  è­¦å‘Š: {filename} æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                continue

    return historical_data


def format_historical_news(historical_data):
    """
    å°†å†å²æ–°é—»æ ¼å¼åŒ–ä¸ºå¯è¯»çš„å­—ç¬¦ä¸²

    Args:
        historical_data: å†å²æ–°é—»æ•°æ®åˆ—è¡¨

    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    if not historical_data:
        return "æ²¡æœ‰æ‰¾åˆ°å†å²æ–°é—»æ–‡ä»¶ã€‚"

    formatted = []
    for day_data in historical_data:
        formatted.append(f"=== {day_data['date']} ===")
        for news_item in day_data['news']:
            content = news_item.get('content', '')
            keywords = ', '.join(news_item.get('keywords', []))
            formatted.append(f"- {content}")
            if keywords:
                formatted.append(f"  å…³é”®è¯: {keywords}")
        formatted.append("")

    return "\n".join(formatted)


def build_prompt(today_date_formatted, historical_news_text):
    """
    æ„å»ºå®Œæ•´çš„Claude prompt

    Args:
        today_date_formatted: ä»Šæ—¥æ—¥æœŸï¼ˆæ ¼å¼åŒ–åï¼‰
        historical_news_text: å†å²æ–°é—»æ–‡æœ¬

    Returns:
        å®Œæ•´çš„promptå­—ç¬¦ä¸²
    """
    json_format_template = """{
  "date": "2025-10-14",
  "date_formatted": "25/10/14",
  "title": "æ—©é¤ AI é€Ÿè¯»",
  "news": [
    {
      "id": 1,
      "title": "æ–°é—»æ ‡é¢˜ï¼ˆä¸è¶…è¿‡15å­—ï¼‰",
      "content": "æ–°é—»è¯¦ç»†å†…å®¹...",
      "importance": "critical",
      "reasoning": "åˆ¤æ–­ç†ç”±...",
      "source": "æ¥æºç½‘ç«™",
      "url": "https://...",
      "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
      "category": "æŠ€æœ¯çªç ´"
    }
  ],
  "metadata": {
    "generated_at": "2025-10-14T08:00:00Z",
    "total_news": 10,
    "sources_checked": ["æ¥æºåˆ—è¡¨"],
    "duplicates_removed": 5,
    "generation_time_seconds": 45.2
  }
}"""

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„AIè¡Œä¸šåˆ†æå¸ˆï¼Œè´Ÿè´£ç”Ÿæˆæ¯æ—¥AIæ–°é—»ç®€æŠ¥ã€‚

## ä»Šæ—¥ä»»åŠ¡
ç”Ÿæˆ {today_date_formatted} çš„AIæ–°é—»ç®€æŠ¥ï¼Œè¾“å‡ºä¸ºJSONæ ¼å¼ã€‚

## ç¬¬ä¸€æ­¥ï¼šæŠ“å–å’Œæ”¶é›†æ–°é—»
ä»ä»¥ä¸‹æ–°é—»æºè·å–æœ€è¿‘24-48å°æ—¶çš„AIç›¸å…³æ–°é—»ï¼š
- AI Breakfast: https://aibreakfast.beehiiv.com
- The Neuron Daily: https://www.theneurondaily.com
- AI News: https://www.artificialintelligence-news.com
- TechCrunch AI: https://techcrunch.com/category/artificial-intelligence/
- The Verge AI: https://www.theverge.com/ai-artificial-intelligence
- VentureBeat AI: https://venturebeat.com/category/ai/
- MIT Tech Review: https://www.technologyreview.com/topic/artificial-intelligence/

## ç¬¬äºŒæ­¥ï¼šå»é‡æ£€æŸ¥
ä»¥ä¸‹æ˜¯æœ€è¿‘7å¤©å·²æŠ¥é“çš„æ–°é—»å†…å®¹ï¼š

{historical_news_text}

è¯·å¯¹æ¯”æ–°æŠ“å–çš„æ–°é—»ï¼Œè¿‡æ»¤æ‰ï¼š
1. å®Œå…¨é‡å¤çš„å†…å®¹ï¼ˆç›¸ä¼¼åº¦ > 85%ï¼‰
2. æ— å®è´¨æ–°è¿›å±•çš„å»¶ç»­æŠ¥é“ï¼ˆç›¸ä¼¼åº¦ 60-85%ï¼‰
3. ä¿ç•™ï¼šä¸åŒè§’åº¦çš„ç›¸ä¼¼è¯é¢˜ã€æœ‰æ–°è¿›å±•çš„åç»­æŠ¥é“

## ç¬¬ä¸‰æ­¥ï¼šè¯„ä¼°å’Œç­›é€‰
å¯¹æ¯æ¡å€™é€‰æ–°é—»ï¼Œæ ¹æ®ä»¥ä¸‹æ¡†æ¶è¿›è¡Œè¯„ä¼°ï¼š

### è¯„ä¼°ç»´åº¦
1. **å½±å“èŒƒå›´**ï¼šè¡Œä¸šçº§ > èµ›é“çº§ > å…¬å¸çº§
2. **åˆ›æ–°ç¨‹åº¦**ï¼šèŒƒå¼è½¬å˜ > æ˜¾è‘—çªç ´ > æ¸è¿›æ”¹è¿› > å¸¸è§„å‘å¸ƒ
3. **æ—¶æ•ˆç´§è¿«æ€§**ï¼šçªå‘äº‹ä»¶ > è¿‘æœŸçƒ­ç‚¹ > æŒç»­è¯é¢˜
4. **å®é™…åº”ç”¨ä»·å€¼**ï¼šå¯è½åœ°æ€§ã€ç”¨æˆ·å½±å“ã€å•†ä¸šä»·å€¼
5. **è¯é¢˜æ€§**ï¼šè®¨è®ºåº¦ã€äº‰è®®æ€§ã€æ„å¤–æ€§

### é‡è¦æ€§åˆ†çº§
- **critical**: æ»¡è¶³å¤šä¸ªé«˜åˆ†ç»´åº¦ï¼Œè¡Œä¸šçº§å½±å“
- **high**: æ»¡è¶³éƒ¨åˆ†é«˜åˆ†ç»´åº¦ï¼Œèµ›é“çº§å½±å“æˆ–é«˜è¯é¢˜æ€§
- **medium**: å€¼å¾—å…³æ³¨çš„å¸¸è§„æ›´æ–°

### ç­›é€‰ç›®æ ‡
é€‰å‡º 8-12 æ¡æœ€å€¼å¾—å…³æ³¨çš„æ–°é—»ï¼Œå…¶ä¸­ï¼š
- critical çº§åˆ«ï¼š1-3æ¡
- high çº§åˆ«ï¼š3-5æ¡
- medium çº§åˆ«ï¼š2-4æ¡

**æ³¨æ„**ï¼šå¦‚æœæŸæ—¥é‡å¤§æ–°é—»ç‰¹åˆ«å¤šï¼Œå¯ä»¥å…¨éƒ¨é€‰critical/highçº§åˆ«ï¼Œä¸å¿…ç¡¬å‡‘mediumã€‚

## ç¬¬å››æ­¥ï¼šæ’°å†™æ–°é—»æè¿°

### æ¯æ¡æ–°é—»éœ€è¦ä¸¤ä¸ªå­—æ®µï¼š

**1. titleï¼ˆæ ‡é¢˜ï¼‰**
- **å¿…é¡»ä¸è¶…è¿‡15ä¸ªå­—**
- ç²¾ç‚¼æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹
- ç”¨å†’å·ç»“å°¾ï¼ˆå¦‚ï¼š"ä¸‰æ˜Ÿå°æ¨¡å‹é¢ è¦†è®¤çŸ¥ï¼š"ï¼‰
- æˆ–ç”¨åè¯çŸ­è¯­ï¼ˆå¦‚ï¼š"AIæ¨¡å‹ä¸­æ¯’é£é™©"ï¼‰

**2. contentï¼ˆå†…å®¹ï¼‰**

### å­—æ•°åˆ†é…
- **critical**: 50-80å­—
  - åŒ…å«ï¼šå…³é”®ä¸»ä½“ + æ ¸å¿ƒå†…å®¹ + èƒŒæ™¯/åŸå›  + å½±å“/æ„ä¹‰

- **high**: 35-50å­—
  - åŒ…å«ï¼šå…³é”®ä¸»ä½“ + æ ¸å¿ƒå†…å®¹ + ç®€è¦å½±å“

- **medium**: 20-35å­—
  - åŒ…å«ï¼šå…³é”®ä¸»ä½“ + æ ¸å¿ƒäº‹å®

### å†™ä½œåŸåˆ™
1. **æ ‡é¢˜ç®€æ´**ï¼štitle å­—æ®µä¸è¶…è¿‡15å­—ï¼Œé«˜åº¦æç‚¼
2. **content ç‹¬ç«‹å®Œæ•´ï¼ˆé‡è¦ï¼ï¼‰**ï¼šcontent å¿…é¡» self-containedï¼Œä¸èƒ½ä¾èµ– titleï¼Œå¿…é¡»åŒ…å«å®Œæ•´ä¸»è¯­å’Œä¸Šä¸‹æ–‡ï¼Œå•ç‹¬é˜…è¯»ä¹Ÿèƒ½å®Œå…¨ç†è§£
3. **ä¸­è‹±æ–‡æ··æ’è§„èŒƒï¼ˆé‡è¦ï¼ï¼‰**ï¼šä¸­æ–‡å’Œè‹±æ–‡/æ•°å­—äº¤æ¥å¤„å¿…é¡»æœ‰ç©ºæ ¼ï¼Œå¦‚"ä¸€ä¸ª AI æ¨¡å‹"ã€"OpenAI å‘å¸ƒ"ã€"Meta æ¨å‡º"
4. **ä¿¡æ¯å¯†åº¦ä¼˜å…ˆ**ï¼šcontent ä¸ä¸ºå‡‘å­—æ•°åŠ åºŸè¯
5. **æ ¸å¿ƒè¦ç´ å®Œæ•´**ï¼š"è°/ä»€ä¹ˆ/ä¸ºä»€ä¹ˆ/å½±å“"
6. **å®¢è§‚ä¸­ç«‹**ï¼šé¿å…è¿‡åº¦å¤¸å¼ 
7. **ä¸“ä¸šæœ¯è¯­é€‚åº¦**ï¼šæ—¢è¦å‡†ç¡®ï¼Œä¹Ÿè¦æ˜“æ‡‚
8. **ä¸­æ–‡æµç•…**ï¼šé¿å…ç¿»è¯‘è…”

## ç¬¬äº”æ­¥ï¼šè¾“å‡ºJSON

æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆç¡®ä¿æ˜¯æœ‰æ•ˆçš„JSONï¼‰ï¼š

{json_format_template}

## é‡è¦æé†’
1. å¿…é¡»è¾“å‡ºæœ‰æ•ˆçš„JSONæ ¼å¼
2. æ‰€æœ‰æ–°é—»æŒ‰importanceæ’åºï¼ˆcriticalåœ¨å‰ï¼Œmediumåœ¨åï¼‰
3. æ¯æ¡æ–°é—»éƒ½è¦æä¾›reasoningï¼Œè¯´æ˜ä¸ºä»€ä¹ˆåˆ¤æ–­ä¸ºè¯¥çº§åˆ«
4. å°½å¯èƒ½æä¾›åŸæ–‡URL
5. keywordsè¦å‡†ç¡®ï¼Œç”¨äºå»é‡å’Œæ ‡ç­¾

è¯·å¼€å§‹æ‰§è¡Œä»»åŠ¡ã€‚ä½ å¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·ï¼ˆå¦‚WebFetchï¼‰æ¥è·å–ç½‘é¡µå†…å®¹ã€‚æœ€åè¯·åªè¾“å‡ºçº¯JSONæ•°æ®ï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"""

    return prompt


def generate_ai_news_brief(args):
    """
    ç”Ÿæˆä»Šæ—¥AIæ–°é—»ç®€æŠ¥

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        ç”Ÿæˆçš„æ–°é—»ç®€æŠ¥JSONæ•°æ®
    """
    try:
        import anthropic
    except ImportError:
        print("âŒ é”™è¯¯: æœªå®‰è£… anthropic åº“")
        print("è¯·è¿è¡Œ: pip install anthropic")
        return None

    # æ£€æŸ¥API Key
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡")
        print("è¯·è¿è¡Œ: export ANTHROPIC_API_KEY='your-api-key'")
        return None

    # åˆå§‹åŒ– Claude å®¢æˆ·ç«¯
    client = anthropic.Anthropic(api_key=api_key)

    # è·å–æ—¥æœŸ
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d")
        except ValueError:
            print("âŒ é”™è¯¯: æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD")
            return None
    else:
        target_date = datetime.now()

    today = target_date.strftime("%Y-%m-%d")
    today_formatted = target_date.strftime("%y/%m/%d")

    # è·å–å†å²æ–°é—»
    print(f"ğŸ“… ç”Ÿæˆæ—¥æœŸ: {today_formatted}")
    print("ğŸ“š è¯»å–å†å²æ–°é—»...")
    historical_data = get_historical_news_json(days=7)
    historical_text = format_historical_news(historical_data)
    print(f"âœ“ å·²è¯»å–æœ€è¿‘ {len(historical_data)} å¤©çš„å†å²è®°å½•")

    # æ„å»º prompt
    prompt = build_prompt(today_formatted, historical_text)

    # è°ƒç”¨ Claude API
    print("ğŸ¤– æ­£åœ¨è°ƒç”¨ Claude API ç”Ÿæˆæ–°é—»ç®€æŠ¥...")
    print("â³ è¿™å¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("-" * 60)

    start_time = datetime.now()

    try:
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=8192,
            temperature=0.7,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        generation_time = (datetime.now() - start_time).total_seconds()

        # æå–ç”Ÿæˆçš„å†…å®¹
        response_text = message.content[0].text

        # å°è¯•æå–JSONï¼ˆå¯èƒ½åŒ…å«åœ¨markdownä»£ç å—ä¸­ï¼‰
        json_text = response_text
        if "```json" in response_text:
            json_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            json_text = response_text.split("```")[1].split("```")[0].strip()

        # è§£æJSON
        news_data = json.loads(json_text)

        # æ›´æ–°metadataä¸­çš„ç”Ÿæˆæ—¶é—´
        if "metadata" in news_data:
            news_data["metadata"]["generation_time_seconds"] = round(generation_time, 2)

        print(f"âœ“ ç”Ÿæˆå®Œæˆï¼è€—æ—¶ {generation_time:.1f} ç§’")
        print("-" * 60)

        # æ˜¾ç¤ºæ‘˜è¦
        total_news = news_data.get("metadata", {}).get("total_news", len(news_data.get("news", [])))
        sources = news_data.get("metadata", {}).get("sources_checked", [])
        duplicates = news_data.get("metadata", {}).get("duplicates_removed", 0)

        print(f"ğŸ“Š æ–°é—»æ€»æ•°: {total_news}")
        print(f"ğŸŒ æ•°æ®æº: {len(sources)} ä¸ª")
        print(f"ğŸ—‘ï¸  å»é‡: {duplicates} æ¡")

        # æŒ‰é‡è¦æ€§ç»Ÿè®¡
        news_list = news_data.get("news", [])
        critical_count = sum(1 for n in news_list if n.get("importance") == "critical")
        high_count = sum(1 for n in news_list if n.get("importance") == "high")
        medium_count = sum(1 for n in news_list if n.get("importance") == "medium")

        print(f"ğŸ”¥ é‡è¦ç¨‹åº¦: Critical({critical_count}) | High({high_count}) | Medium({medium_count})")
        print("-" * 60)

        return news_data, today

    except anthropic.APIError as e:
        print(f"âŒ API é”™è¯¯: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æé”™è¯¯: {e}")
        print("å“åº”å†…å®¹:")
        print(response_text[:500])
        return None
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        return None


def save_news_data(news_data, today, args):
    """
    ä¿å­˜æ–°é—»æ•°æ®åˆ°JSONå’ŒTXTæ–‡ä»¶

    Args:
        news_data: æ–°é—»JSONæ•°æ®
        today: æ—¥æœŸå­—ç¬¦ä¸²
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    # ä¿å­˜ JSON æ–‡ä»¶
    json_file = f"{today}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ JSON å·²ä¿å­˜: {json_file}")

    # åŒæ—¶ä¿å­˜ä¸º TXT æ ¼å¼ï¼ˆç”¨äºå¿«é€ŸæŸ¥çœ‹ï¼‰
    txt_file = f"{today}.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(f"{news_data['title']} - {news_data['date_formatted']}\n")
        f.write("=" * 60 + "\n\n")

        for news_item in news_data.get('news', []):
            importance_icon = {
                'critical': 'ğŸ”¥',
                'high': 'ğŸ“Œ',
                'medium': 'Â·'
            }.get(news_item.get('importance', 'medium'), 'Â·')

            f.write(f"{importance_icon} {news_item['content']}\n")
            if news_item.get('url'):
                f.write(f"   {news_item['url']}\n")
            f.write("\n")

        f.write("-" * 60 + "\n")
        metadata = news_data.get('metadata', {})
        f.write(f"ç”Ÿæˆæ—¶é—´: {metadata.get('generated_at', '')}\n")
        f.write(f"æ–°é—»æ€»æ•°: {metadata.get('total_news', 0)}\n")
        f.write(f"æ•°æ®æº: {', '.join(metadata.get('sources_checked', []))}\n")

    print(f"âœ“ TXT å·²ä¿å­˜: {txt_file}")

    # æç¤ºç”¨æˆ·ä½¿ç”¨å¼€å‘æœåŠ¡å™¨
    if not args.no_browser:
        print("\n" + "=" * 60)
        print("ğŸ“± ç°åœ¨å¯ä»¥é€šè¿‡å¼€å‘æœåŠ¡å™¨æŸ¥çœ‹æ–°é—»:")
        print("   npm run dev")
        print("=" * 60)


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(
        description='AI æ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python generate_news.py                    # ç”Ÿæˆä»Šæ—¥ç®€æŠ¥
  python generate_news.py --date 2025-10-13  # ç”ŸæˆæŒ‡å®šæ—¥æœŸç®€æŠ¥
  python generate_news.py --no-browser       # ç”Ÿæˆä½†ä¸æ‰“å¼€æµè§ˆå™¨
  python generate_news.py --count 15         # è‡ªå®šä¹‰æ–°é—»æ•°é‡
        """
    )

    parser.add_argument(
        '--date',
        type=str,
        help='æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©'
    )

    parser.add_argument(
        '--no-browser',
        action='store_true',
        help='ä¸è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨é¢„è§ˆ'
    )

    parser.add_argument(
        '--count',
        type=int,
        default=10,
        help='ç›®æ ‡æ–°é—»æ•°é‡ (é»˜è®¤: 10)'
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸŒ… AI æ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨")
    print("=" * 60)

    # ç”Ÿæˆæ–°é—»ç®€æŠ¥
    result = generate_ai_news_brief(args)

    if result:
        news_data, today = result
        save_news_data(news_data, today, args)
        print("=" * 60)
        print("âœ… ç”Ÿæˆå®Œæˆï¼")
        print("=" * 60)
    else:
        print("âŒ ç”Ÿæˆå¤±è´¥")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
